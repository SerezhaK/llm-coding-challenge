{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_fgC7NX1nd",
        "outputId": "d7a04727-1dde-452e-b50a-178e9028b173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPq3gdYgYSAL",
        "outputId": "20a1f4d2-a9ee-4ed1-f1b6-d51541024d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z57Ej5vAXnpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1047de81-fa17-4125-f8b8-3e70f3f4e0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting STRUCTURED full pull request data fetch for AlfaInsurance/devQ_testData_PythonProject\n",
            "Target PR state: all\n",
            "Output directory: pull_request_data_structured\n",
            "Ensure GITHUB_BOT_ACCESS_TOKEN environment variable is set.\n",
            "WARNING: This can take a long time and consume significant disk space and API calls.\n",
            "--- Starting STRUCTURED pull request data fetch for AlfaInsurance/devQ_testData_PythonProject ---\n",
            "--- Output base directory: pull_request_data_structured ---\n",
            "\n",
            "Fetching page 1 of pull requests list from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls...\n",
            "Processing 2 pull requests from page 1...\n",
            "\n",
            "--- Processing PR #1: Hackaton ---\n",
            "    Updated at: 2025-04-14T09:00:47Z\n",
            "    Fetching full details for PR #1...\n",
            "    Fetching changed files for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/files...\n",
            "    Processing 16 files for PR #1...\n",
            "      Processing file: .gitlab-ci.yml (Status: removed)\n",
            "        Fetching content: .gitlab-ci.yml @ c916a6d\n",
            "      Processing file: accesslist/templates/acl_create_info.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_demo.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_dmz_resources.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ c916a6d\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ 28b7d14\n",
            "      Processing file: accesslist/utils/gitlab.py (Status: modified)\n",
            "        Fetching content: accesslist/utils/gitlab.py @ c916a6d\n",
            "HTTP error making API request to https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/accesslist/utils/gitlab.py?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e: 404 Client Error: Not Found for url: https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/accesslist/utils/gitlab.py?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e\n",
            "Response Status: 404\n",
            "Response Body: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/contents#get-repository-content\",\"status\":\"404\"}\n",
            "        Fetching content: accesslist/utils/gitlab.py @ 28b7d14\n",
            "      Processing file: accesslist/views.py (Status: modified)\n",
            "        Fetching content: accesslist/views.py @ c916a6d\n",
            "        Fetching content: accesslist/views.py @ 28b7d14\n",
            "      Processing file: acladmin/settings.py (Status: modified)\n",
            "        Fetching content: acladmin/settings.py @ c916a6d\n",
            "        Fetching content: acladmin/settings.py @ 28b7d14\n",
            "      Processing file: acladmin/tasks.py (Status: modified)\n",
            "        Fetching content: acladmin/tasks.py @ c916a6d\n",
            "        Fetching content: acladmin/tasks.py @ 28b7d14\n",
            "      Processing file: acladmin/wsgi.py (Status: modified)\n",
            "        Fetching content: acladmin/wsgi.py @ c916a6d\n",
            "        Fetching content: acladmin/wsgi.py @ 28b7d14\n",
            "      Processing file: init.sh (Status: removed)\n",
            "        Fetching content: init.sh @ c916a6d\n",
            "HTTP error making API request to https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/init.sh?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e: 404 Client Error: Not Found for url: https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/contents/init.sh?ref=c916a6d807e86e2f88a38d1f960dedae53d8e97e\n",
            "Response Status: 404\n",
            "Response Body: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/contents#get-repository-content\",\"status\":\"404\"}\n",
            "      Processing file: ownerlist/templates/registration/login.html (Status: modified)\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ c916a6d\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ 28b7d14\n",
            "      Processing file: ownerlist/utils.py (Status: modified)\n",
            "        Fetching content: ownerlist/utils.py @ c916a6d\n",
            "        Fetching content: ownerlist/utils.py @ 28b7d14\n",
            "      Processing file: panel/templates/panel.html (Status: modified)\n",
            "        Fetching content: panel/templates/panel.html @ c916a6d\n",
            "        Fetching content: panel/templates/panel.html @ 28b7d14\n",
            "      Processing file: panel/views.py (Status: modified)\n",
            "        Fetching content: panel/views.py @ c916a6d\n",
            "        Fetching content: panel/views.py @ 28b7d14\n",
            "      Processing file: requirements.txt (Status: modified)\n",
            "        Fetching content: requirements.txt @ c916a6d\n",
            "        Fetching content: requirements.txt @ 28b7d14\n",
            "      Processing file: templates/base.html (Status: modified)\n",
            "        Fetching content: templates/base.html @ c916a6d\n",
            "        Fetching content: templates/base.html @ 28b7d14\n",
            "    Fetching reviews for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/reviews...\n",
            "    Fetching review comments for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/comments...\n",
            "    Fetching issue comments for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/issues/1/comments...\n",
            "    Fetching commits for PR #1...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/1/commits...\n",
            "      Fetching page 2 from https://api.github.com/repositories/964502314/pulls/1/commits...\n",
            "      Fetching page 3 from https://api.github.com/repositories/964502314/pulls/1/commits...\n",
            "    Fetching check runs for commit 28b7d14...\n",
            "    Fetching statuses for commit 28b7d14...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/commits/28b7d14401ca39872a6922ac3df25e1ef95c0750/statuses...\n",
            "    Successfully saved metadata to pull_request_data_structured/pr_1/metadata.json\n",
            "\n",
            "--- Processing PR #2: v1 ---\n",
            "    Updated at: 2025-04-11T10:14:27Z\n",
            "    Fetching full details for PR #2...\n",
            "    Fetching changed files for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/files...\n",
            "    Processing 16 files for PR #2...\n",
            "      Processing file: .gitlab-ci.yml (Status: removed)\n",
            "        Fetching content: .gitlab-ci.yml @ a32789d\n",
            "      Processing file: accesslist/templates/acl_create_info.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_create_info.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_demo.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_demo.html @ 28b7d14\n",
            "      Processing file: accesslist/templates/acl_dmz_resources.html (Status: modified)\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ a32789d\n",
            "        Fetching content: accesslist/templates/acl_dmz_resources.html @ 28b7d14\n",
            "      Processing file: accesslist/utils/gitlab.py (Status: modified)\n",
            "        Fetching content: accesslist/utils/gitlab.py @ a32789d\n",
            "        Fetching content: accesslist/utils/gitlab.py @ 28b7d14\n",
            "      Processing file: accesslist/views.py (Status: modified)\n",
            "        Fetching content: accesslist/views.py @ a32789d\n",
            "        Fetching content: accesslist/views.py @ 28b7d14\n",
            "      Processing file: acladmin/settings.py (Status: modified)\n",
            "        Fetching content: acladmin/settings.py @ a32789d\n",
            "        Fetching content: acladmin/settings.py @ 28b7d14\n",
            "      Processing file: acladmin/tasks.py (Status: modified)\n",
            "        Fetching content: acladmin/tasks.py @ a32789d\n",
            "        Fetching content: acladmin/tasks.py @ 28b7d14\n",
            "      Processing file: acladmin/wsgi.py (Status: modified)\n",
            "        Fetching content: acladmin/wsgi.py @ a32789d\n",
            "        Fetching content: acladmin/wsgi.py @ 28b7d14\n",
            "      Processing file: init.sh (Status: removed)\n",
            "        Fetching content: init.sh @ a32789d\n",
            "      Processing file: ownerlist/templates/registration/login.html (Status: modified)\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ a32789d\n",
            "        Fetching content: ownerlist/templates/registration/login.html @ 28b7d14\n",
            "      Processing file: ownerlist/utils.py (Status: modified)\n",
            "        Fetching content: ownerlist/utils.py @ a32789d\n",
            "        Fetching content: ownerlist/utils.py @ 28b7d14\n",
            "      Processing file: panel/templates/panel.html (Status: modified)\n",
            "        Fetching content: panel/templates/panel.html @ a32789d\n",
            "        Fetching content: panel/templates/panel.html @ 28b7d14\n",
            "      Processing file: panel/views.py (Status: modified)\n",
            "        Fetching content: panel/views.py @ a32789d\n",
            "        Fetching content: panel/views.py @ 28b7d14\n",
            "      Processing file: requirements.txt (Status: modified)\n",
            "        Fetching content: requirements.txt @ a32789d\n",
            "        Fetching content: requirements.txt @ 28b7d14\n",
            "      Processing file: templates/base.html (Status: modified)\n",
            "        Fetching content: templates/base.html @ a32789d\n",
            "        Fetching content: templates/base.html @ 28b7d14\n",
            "    Fetching reviews for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/reviews...\n",
            "    Fetching review comments for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/comments...\n",
            "    Fetching issue comments for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/issues/2/comments...\n",
            "    Fetching commits for PR #2...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/pulls/2/commits...\n",
            "    Fetching check runs for commit 28b7d14...\n",
            "    Fetching statuses for commit 28b7d14...\n",
            "      Fetching page 1 from https://api.github.com/repos/AlfaInsurance/devQ_testData_PythonProject/commits/28b7d14401ca39872a6922ac3df25e1ef95c0750/statuses...\n",
            "    Successfully saved metadata to pull_request_data_structured/pr_2/metadata.json\n",
            "No 'next' link found in PR list response, reached the last page.\n",
            "\n",
            "--- Finished STRUCTURED processing for AlfaInsurance/devQ_testData_PythonProject. ---\n",
            "--- Processed 2 pull requests. ---\n",
            "--- Data saved in subdirectories within: pull_request_data_structured ---\n",
            "\n",
            "--------------------------------------------------\n",
            "Successfully finished processing.\n",
            "Processed 2 pull requests.\n",
            "Data saved in 'pull_request_data_structured' directory, organized by PR number.\n",
            "Total execution time: 17.17 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Starting RAG Analysis ---\n",
            "Initializing embeddings model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name microsoft/graphcodebert-base. Creating a new one with mean pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings model initialized.\n",
            "Initializing LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "Cleaning up temporary Chroma directories...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM initialized successfully.\n",
            "\n",
            "\u001b[1m--- Analyzing PR #1 ---\u001b[0m\n",
            "\n",
            "Вопрос: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "PR #1 data not loaded. Attempting to load...\n",
            "Processing data for PR #1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files for PR #1: 100%|██████████| 16/16 [00:00<00:00, 252.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created Chroma DB for PR #1 in temporary directory: /tmp/chroma_db_pr_1__ygkg6sb\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: 1. Code changes (diff): The PR contains no code changes. Therefore, there are no potential vulnerabilities in these changes.\n",
            "\n",
            "            2. Developer comments (issue and review comments): The PR mentions no developer comments or issues related to security. Therefore, there are no potential vulnerabilities in these comments.\n",
            "\n",
            "            3. Results of CI checks: The PR does not mention any CI checks. Therefore, there are no potential vulnerabilities in these checks.\n",
            "\n",
            "            4. Commit history (summarized in context): The commit history summarized in the context shows that the PR has been merged into the main branch successfully. Therefore, there are no potential vulnerabilities in this context.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Соответствует ли код стандартам проекта?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Соответствует ли код стандартам проекта?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: Yes, the code in the given PR meets the project standards. The PR follows the established coding conventions and best practices for Python programming. It also includes appropriate comments and documentation to explain the purpose and functionality of each function or class. Additionally, the code passes all unit tests and has been thoroughly tested by the developer before submission.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Есть ли проблемы с производительностью в измененном коде?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Есть ли проблемы с производительностью в измененном коде?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: Да, есть возможность обнаружить некоторые проблемы с производительностью в измененном коде. Это связано с тем, что изменения могут привести к увеличению размера файлов и переполнению памяти, что может повлиять на производительность при выполнении программного кода. В случае, если вы заметите, что изменения в коде приводят к увеличению размера файлов или переполнению памяти, то следует проверить, какие изменения были внесены, чтобы определить, какая из них привела к проблемам. Также, можно использовать инструменты для проверки производительности, такие как `pyperf` или `cProfile`.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Summarize the main changes in PR 1.\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: Summarize the main changes in PR 1.\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #1 - Hackaton ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #1 - Hackaton ---\n",
            "Author: Vasilev...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: PR 1 made significant changes to the base template file and added a new template file with additional features. The changes include adding a new section for access control lists (ACLs), which allows users to specify which files or directories they can access. Additionally, there were no CI checks found in the pull request, indicating that the developer did not perform any automated testing or validation before submitting the PR.\n",
            "Источники: [{'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: What were the CI check results for PR 2?\n",
            "--- Debugging Prompt Variables for PR #1 ---\n",
            "pr_number_str: 1\n",
            "question: What were the CI check results for PR 2?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #1 - Hackaton ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #1 - Hackaton ---\n",
            "Author: Vasilev...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: The CI check results for PR 2 are not provided in the given text. Please provide them yourself or refer to the given text for more details.\n",
            "Источники: [{'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #1 - Hackaton ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "\u001b[1m--- Analyzing PR #2 ---\u001b[0m\n",
            "\n",
            "Вопрос: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "PR #2 data not loaded. Attempting to load...\n",
            "Processing data for PR #2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files for PR #2: 100%|██████████| 16/16 [00:00<00:00, 365.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created Chroma DB for PR #2 in temporary directory: /tmp/chroma_db_pr_2_uzi57pen\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Какие потенциальные уязвимости есть в этих изменениях?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: 1. Code changes (diff): The PR contains no code changes. Therefore, there are no potential vulnerabilities in these changes.\n",
            "\n",
            "            2. Developer comments (issue and review comments): The PR mentions no developer comments or issues related to security. Therefore, there are no potential vulnerabilities in these comments.\n",
            "\n",
            "            3. Results of CI checks: The PR does not mention any CI checks. Therefore, there are no potential vulnerabilities in these checks.\n",
            "\n",
            "            4. Commit history (summarized in context): The commit history summarized in the context shows that the PR has been merged into the main branch successfully. Therefore, there are no potential vulnerabilities in this context.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Соответствует ли код стандартам проекта?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Соответствует ли код стандартам проекта?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: Yes, the code in the given PR meets the project standards. The PR follows the established coding conventions and best practices for Python programming. It also includes appropriate comments and documentation to explain the purpose and functionality of each function or class. Additionally, the code passes all unit tests and has been thoroughly tested by the developer before submission.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Есть ли проблемы с производительностью в измененном коде?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Есть ли проблемы с производительностью в измененном коде?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): def task(request, acl_id) -> bool:\n",
            "    return\n",
            "    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\n",
            "    logger.info(\n",
            "        f\"[Отправка в omni] Начинается выполнение...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: Да, есть возможность заметить некоторые проблемы с производительностью в измененном коде. Это может быть связано с различными причинами, такими как:\n",
            "\n",
            "1. Увеличение размера файлов или переменных, что приводит к увеличению времени загрузки и запуска программы.\n",
            "2. Использование больших массивов или списков, которые могут привести к выделению памяти и повышению требований к ресурсам.\n",
            "3. Введение новых функций или переименования существующих, что приводит к необходимости перечисления всех функций и переименованию их в более подходящих названиях.\n",
            "4. Применение новых технологий, которые могут привести к увеличению времени работы программы.\n",
            "5. Ограничение количества оперативной памяти, которое может привести к тому, что программа работает медленнее.\n",
            "\n",
            "Если вы заметите эти проблемы, то следует проверить, какие из них возникают в изменённом коде, чтобы определить, какая из них является основной причиной. После этого можно принять меры по устранению этих проблем, чтобы повысить производительность кода.\n",
            "Источники: [{'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': 'def task(request, acl_id) -> bool:\\n    return\\n    \"\"\"Функция обработки запросов на выполнение активностей для выполнения обращения\"\"\"\\n    logger.info(\\n        f\"[Отправка в omni] Начинается выполнение...', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: Summarize the main changes in PR 1.\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: Summarize the main changes in PR 1.\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: ...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: PR 1 made significant changes to the base template file, including adding new variables and modifying existing ones. The author also added new HTML elements and updated existing ones to improve the overall layout and functionality of the template. Additionally, the author included developer comments and CI checks for the head commit, but these were not included in the context provided. Overall, the changes made by the author aimed to enhance the template's usability and functionality.\n",
            "Источники: [{'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "Вопрос: What were the CI check results for PR 2?\n",
            "--- Debugging Prompt Variables for PR #2 ---\n",
            "pr_number_str: 2\n",
            "question: What were the CI check results for PR 2?\n",
            "ci_checks_str: No CI checks found.\n",
            "context_text (first 200 chars): --- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: templates/base.html\n",
            "Status: closed\n",
            "CI Checks for head commit: No CI checks found.\n",
            "\n",
            "---\n",
            "\n",
            "--- Pull Request #2 - v1 ---\n",
            "Author: VasilevArtem\n",
            "File: ...\n",
            "--- End Debugging Print Statements ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ответ: The given text doesn't provide any specific information about the CI check results for PR 2. It only mentions that there are no CI checks found for the head commit.\n",
            "Источники: [{'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: templates/base.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}, {'content_snippet': '--- Pull Request #2 - v1 ---\\nAuthor: VasilevArtem\\nFile: accesslist/templates/acl_demo.html\\nStatus: closed\\nCI Checks for head commit: No CI checks found....', 'file': 'Unknown (metadata not stored)', 'checks': 'Unknown (metadata not stored)', 'author': 'Unknown (metadata not stored)'}]\n",
            "\n",
            "--- RAG Analysis Finished ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "import tempfile # Import tempfile module\n",
        "import shutil # Import shutil for cleaning up temporary directories\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Set this environment variable before running the script!\n",
        "# Example: export GITHUB_BOT_ACCESS_TOKEN='your_github_token_here'\n",
        "# Replace with your actual token or load from environment securely\n",
        "github_secret = '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' # Consider more secure storage\n",
        "# It's highly recommended to load the token from environment variables for security\n",
        "# github_secret = os.environ.get('GITHUB_BOT_ACCESS_TOKEN')\n",
        "\n",
        "if not github_secret:\n",
        "    print(\"WARNING: GITHUB_BOT_ACCESS_TOKEN environment variable not set.\")\n",
        "    print(\"Please set it before running the script for data fetching.\")\n",
        "    # If data fetching is required, uncomment the line below:\n",
        "    # raise ValueError(\"GITHUB_BOT_BOT_ACCESS_TOKEN environment variable not set.\")\n",
        "\n",
        "\n",
        "# Main output directory where PR-specific folders will be created\n",
        "OUTPUT_DIR_BASE = \"pull_request_data_structured\"\n",
        "API_VERSION = '2022-11-28'\n",
        "PER_PAGE = 100 # Max items per page for pagination\n",
        "REQUEST_TIMEOUT = 60 # Seconds for API request timeout\n",
        "# REQUEST_DELAY = 1 # Seconds delay between API requests (Uncomment and adjust for rate limiting)\n",
        "\n",
        "# --- Helper Functions for GitHub API Interaction ---\n",
        "\n",
        "def make_api_request(url, headers, params=None):\n",
        "    \"\"\"\n",
        "    Helper function to make GET requests to the GitHub API and handle common errors.\n",
        "    Includes basic error handling for timeouts, HTTP errors, and request exceptions.\n",
        "    Args:\n",
        "        url (str): The API endpoint URL.\n",
        "        headers (dict): Request headers, including Authorization and Accept.\n",
        "        params (dict, optional): URL parameters. Defaults to None.\n",
        "    Returns:\n",
        "        requests.Response: The response object if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    # Ensure the correct API version is specified in headers\n",
        "    headers['X-GitHub-Api-Version'] = API_VERSION\n",
        "    try:\n",
        "        # time.sleep(REQUEST_DELAY) # Uncomment and adjust for rate limiting\n",
        "        response = requests.get(url, headers=headers, params=params, timeout=REQUEST_TIMEOUT)\n",
        "\n",
        "        # Optional: Basic rate limit check (prints remaining requests)\n",
        "        # if 'X-RateLimit-Remaining' in response.headers:\n",
        "        #     print(f\"      API Rate Limit Remaining: {response.headers['X-RateLimit-Remaining']}\")\n",
        "\n",
        "        response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n",
        "        return response\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Timeout error making API request to {url}\")\n",
        "        return None\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP error making API request to {url}: {e}\")\n",
        "        print(f\"Response Status: {e.response.status_code}\")\n",
        "        print(f\"Response Body: {e.response.text}\")\n",
        "        # Depending on needs, you might want to raise the exception or handle it differently\n",
        "        # raise e\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request to {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during API request to {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_paginated_data(url, headers, params=None):\n",
        "    \"\"\"\n",
        "    Fetches all pages for a given paginated GitHub API endpoint.\n",
        "    Handles 'next' links in the response headers for pagination.\n",
        "    Args:\n",
        "        url (str): The initial API endpoint URL.\n",
        "        headers (dict): Request headers.\n",
        "        params (dict, optional): Initial URL parameters. Defaults to None.\n",
        "    Returns:\n",
        "        list: A list containing all items from all pages. Returns empty list on error.\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "    params['per_page'] = PER_PAGE\n",
        "    all_items = []\n",
        "    current_url = url\n",
        "    page = 1\n",
        "\n",
        "    while current_url:\n",
        "        # Ensure the URL format is correct (remove potential brackets)\n",
        "        cleaned_url = current_url.replace('[https://', 'https://').replace(']', '')\n",
        "        print(f\"      Fetching page {page} from {cleaned_url.split('?')[0]}...\")\n",
        "\n",
        "        # Use initial params only on the very first request URL if it doesn't have query string\n",
        "        # Otherwise, the 'next' URL from headers includes all necessary parameters\n",
        "        page_params = params if page == 1 and '?' not in cleaned_url else None\n",
        "\n",
        "        response = make_api_request(cleaned_url, headers=headers, params=page_params)\n",
        "        page += 1\n",
        "\n",
        "        if not response:\n",
        "            print(\"Failed to fetch paginated data page. Stopping pagination.\")\n",
        "            break # Exit if API request fails\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                items_page = response.json()\n",
        "                if not items_page: # Stop if page is empty\n",
        "                    break\n",
        "                if isinstance(items_page, list):\n",
        "                    all_items.extend(items_page)\n",
        "                else:\n",
        "                    # Handle cases where the response might not be a list (unlikely for paginated endpoints)\n",
        "                    print(f\"      Warning: Expected a list from {cleaned_url.split('?')[0]}, received type {type(items_page)}. Appending.\")\n",
        "                    all_items.append(items_page) # Append even if not a list, might be useful\n",
        "\n",
        "                # Follow the 'next' link for pagination\n",
        "                if 'next' in response.links:\n",
        "                    current_url = response.links['next']['url']\n",
        "                    params = None # Params are included in the 'next' URL for subsequent pages\n",
        "                else:\n",
        "                    current_url = None # No more pages\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"      Error decoding JSON from {cleaned_url.split('?')[0] if cleaned_url else url}: {e}\")\n",
        "                print(f\"      Response text: {response.text[:200]}...\") # Print start of text for debugging\n",
        "                break # Stop pagination on JSON error\n",
        "            except Exception as e:\n",
        "                 print(f\"      An unexpected error occurred processing page {page-1} data: {e}\")\n",
        "                 break # Stop pagination on unexpected error\n",
        "\n",
        "        else: # Handle non-200 status for a page\n",
        "            print(f\"Stopping pagination. Received status {response.status_code} for page {page-1}.\")\n",
        "            break # Exit loop if fetching a page fails\n",
        "\n",
        "    return all_items\n",
        "\n",
        "def get_file_content(owner, repo, file_path, commit_sha, headers):\n",
        "    \"\"\"\n",
        "    Get the decoded content of a file at a specific commit SHA.\n",
        "    Returns the decoded content as a string or None if an error occurs or if it's not a file.\n",
        "    Args:\n",
        "        owner (str): Repository owner.\n",
        "        repo (str): Repository name.\n",
        "        file_path (str): Path to the file in the repository.\n",
        "        commit_sha (str): The commit SHA to fetch the file from.\n",
        "        headers (dict): Request headers.\n",
        "    Returns:\n",
        "        str: The decoded file content as a string, or empty string if not found/error.\n",
        "    \"\"\"\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}?ref={commit_sha}\"\n",
        "    print(f\"        Fetching content: {file_path} @ {commit_sha[:7]}\")\n",
        "    response = make_api_request(api_url, headers=headers)\n",
        "\n",
        "    if response and response.status_code == 200:\n",
        "        try:\n",
        "            content_data = response.json()\n",
        "            if isinstance(content_data, dict) and content_data.get('type') == 'file' and 'content' in content_data:\n",
        "                if content_data.get('encoding') == 'base64':\n",
        "                    try:\n",
        "                        encoded_content = content_data['content'].replace('\\n', '')\n",
        "                        decoded_content = base64.b64decode(encoded_content).decode('utf-8', errors='replace')\n",
        "                        return decoded_content\n",
        "                    except Exception as e:\n",
        "                        print(f\"        Error decoding base64 content for {file_path} @ {commit_sha[:7]}: {e}\")\n",
        "                        return \"\" # Return empty string on decoding error\n",
        "                else:\n",
        "                     # Handle cases where content might not be base64 but is present (less common)\n",
        "                     print(f\"        Warning: Content for {file_path} @ {commit_sha[:7]} not base64 encoded, returning raw.\")\n",
        "                     return content_data['content']\n",
        "            elif isinstance(content_data, dict) and content_data.get('type') in ['dir', 'submodule', 'symlink']:\n",
        "                 print(f\"        Skipping content fetch for non-file type '{content_data.get('type')}' for {file_path} @ {commit_sha[:7]}.\")\n",
        "                 return \"\" # Return empty string for non-file types\n",
        "            else:\n",
        "                print(f\"        Warning: Could not get file content (unexpected format or missing content) for {file_path} @ {commit_sha[:7]}. Response type: {type(content_data)}. Content type: {content_data.get('type') if isinstance(content_data, dict) else 'N/A'}\")\n",
        "                return \"\" # Return empty string on unexpected format\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"        Error decoding JSON response for file content {file_path} @ {commit_sha[:7]}: {e}\")\n",
        "            return \"\" # Return empty string on JSON error\n",
        "    elif response and response.status_code == 404:\n",
        "        print(f\"        File not found (404): {file_path} @ {commit_sha[:7]}\")\n",
        "        return \"\" # File doesn't exist at this SHA, which is expected for added/deleted files\n",
        "    else:\n",
        "        # Other errors handled by make_api_request, but we return empty string here\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def save_file(content, base_dir, relative_path):\n",
        "    \"\"\"\n",
        "    Saves content to a file, creating necessary subdirectories.\n",
        "    Returns True on success, False on failure.\n",
        "    Args:\n",
        "        content (str): The content to save.\n",
        "        base_dir (str): The base directory to save files in.\n",
        "        relative_path (str): The path relative to the base_dir.\n",
        "    Returns:\n",
        "        bool: True if the file was saved successfully, False otherwise.\n",
        "    \"\"\"\n",
        "    if content is None or content == \"\": # Don't save if content is None or empty\n",
        "        # print(f\"          Skipping save for empty/None content: {relative_path}\") # Uncomment for detailed logs\n",
        "        return False\n",
        "    try:\n",
        "        # Construct the full path\n",
        "        full_path = os.path.join(base_dir, relative_path)\n",
        "\n",
        "        # Create parent directories if they don't exist\n",
        "        parent_dir = os.path.dirname(full_path)\n",
        "        if parent_dir: # Avoid creating '.' if path has no directory part\n",
        "            os.makedirs(parent_dir, exist_ok=True)\n",
        "\n",
        "        # Write the content to the file\n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        # print(f\"          Successfully saved: {full_path}\") # Uncomment for detailed logs\n",
        "        return True\n",
        "    except IOError as e:\n",
        "        print(f\"          Error writing file {full_path}: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"          Unexpected error saving file {full_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_pull_request_files(owner, repo, pr_number, base_sha, head_sha, headers, pr_output_dir):\n",
        "    \"\"\"\n",
        "    Fetches changed files for a PR, saves their content (before/after) and patches to disk,\n",
        "    and returns a list of file metadata (excluding the large content fields).\n",
        "    Args:\n",
        "        owner (str): Repository owner.\n",
        "        repo (str): Repository name.\n",
        "        pr_number (int): The pull request number.\n",
        "        base_sha (str): The commit SHA of the base branch.\n",
        "        head_sha (str): The commit SHA of the head branch.\n",
        "        headers (dict): Request headers.\n",
        "        pr_output_dir (str): The directory for this specific PR's data.\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing metadata for a changed file.\n",
        "    \"\"\"\n",
        "    print(f\"    Fetching changed files for PR #{pr_number}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
        "    files_list = fetch_paginated_data(api_url, headers=headers)\n",
        "\n",
        "    if not files_list:\n",
        "        print(f\"    No files found or error fetching files for PR #{pr_number}.\")\n",
        "        return [] # Return empty list if no files or error\n",
        "\n",
        "    print(f\"    Processing {len(files_list)} files for PR #{pr_number}...\")\n",
        "\n",
        "    # Create subdirectories for this PR\n",
        "    before_dir = os.path.join(pr_output_dir, \"before_merge\")\n",
        "    after_dir = os.path.join(pr_output_dir, \"after_merge\")\n",
        "    patch_dir = os.path.join(pr_output_dir, \"changed_files\")\n",
        "    os.makedirs(before_dir, exist_ok=True)\n",
        "    os.makedirs(after_dir, exist_ok=True)\n",
        "    os.makedirs(patch_dir, exist_ok=True)\n",
        "\n",
        "    processed_files_metadata = []\n",
        "    for f in files_list:\n",
        "        # Ensure f is a dictionary before processing\n",
        "        if not isinstance(f, dict):\n",
        "             print(f\"Warning: Skipping unexpected item in files list (not a dictionary) for PR #{pr_number}: {f}\")\n",
        "             continue\n",
        "\n",
        "        filename = f.get('filename')\n",
        "        status = f.get('status')\n",
        "\n",
        "        if not filename or not status:\n",
        "             print(f\"Warning: Skipping file entry with missing filename or status for PR #{pr_number}: {f}\")\n",
        "             continue\n",
        "\n",
        "        print(f\"      Processing file: {filename} (Status: {status})\")\n",
        "\n",
        "        # --- Get Content Before (Base) ---\n",
        "        content_base = \"\" # Initialize as empty string\n",
        "        # No base content for added files, and ensure base_sha exists\n",
        "        if status != 'added' and base_sha:\n",
        "            content_base = get_file_content(owner, repo, filename, base_sha, headers)\n",
        "            if content_base: # Only save if content was successfully fetched and is not empty\n",
        "                 save_file(content_base, before_dir, filename)\n",
        "\n",
        "        # --- Get Content After (Head) ---\n",
        "        content_head = \"\" # Initialize as empty string\n",
        "        # No head content for deleted files, and ensure head_sha exists\n",
        "        # 'removed' status is used by GitHub API for deleted files in PR file list\n",
        "        if status not in ['removed', 'deleted'] and head_sha:\n",
        "            content_head = get_file_content(owner, repo, filename, head_sha, headers)\n",
        "            if content_head: # Only save if content was successfully fetched and is not empty\n",
        "                save_file(content_head, after_dir, filename)\n",
        "\n",
        "        # --- Save Patch ---\n",
        "        patch_content = f.get('patch')\n",
        "        if patch_content:\n",
        "            patch_filename = filename + \".patch\"\n",
        "            save_file(patch_content, patch_dir, patch_filename)\n",
        "\n",
        "        # --- Store Metadata (without large content fields) ---\n",
        "        processed_files_metadata.append({\n",
        "            'filename': filename,\n",
        "            'status': status,\n",
        "            'additions': f.get('additions', 0), # Use .get with default for safety\n",
        "            'deletions': f.get('deletions', 0), # Use .get with default for safety\n",
        "            'changes': f.get('changes', 0),     # Use .get with default for safety\n",
        "            'sha': f.get('sha'), # SHA of the file blob in the head commit\n",
        "            'blob_url': f.get('blob_url'),\n",
        "            'raw_url': f.get('raw_url'),\n",
        "            'patch_saved': bool(patch_content), # Indicate if patch was available and saved attempt was made\n",
        "            'content_base_saved': bool(content_base), # Indicate if base content was fetched successfully and was not empty\n",
        "            'content_head_saved': bool(content_head), # Indicate if head content was fetched successfully and was not empty\n",
        "            'previous_filename': f.get('previous_filename') # For renamed files\n",
        "        })\n",
        "\n",
        "    return processed_files_metadata\n",
        "\n",
        "\n",
        "def get_pr_reviews(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all reviews for a PR. Returns a list of simplified review dictionaries.\"\"\"\n",
        "    print(f\"    Fetching reviews for PR #{pr_number}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\"\n",
        "    reviews = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not reviews: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': r.get('id'),\n",
        "            'user': r.get('user', {}).get('login', 'ghost'), # Use .get for nested access safety\n",
        "            'state': r.get('state'),\n",
        "            'submitted_at': r.get('submitted_at'),\n",
        "            'body': r.get('body'), # Full body\n",
        "            'commit_id': r.get('commit_id')\n",
        "        } for r in reviews if isinstance(r, dict) # Ensure item is a dictionary\n",
        "    ]\n",
        "\n",
        "def get_pr_review_comments(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all review comments (inline code comments) for a PR. Returns a list of simplified comment dictionaries.\"\"\"\n",
        "    print(f\"    Fetching review comments for PR #{pr_number}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n",
        "    comments = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not comments: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': c.get('id'),\n",
        "            'user': c.get('user', {}).get('login', 'ghost'), # Use .get for nested access safety\n",
        "            'body': c.get('body'), # Full body\n",
        "            'path': c.get('path'), # File path the comment refers to\n",
        "            'position': c.get('position'), # Line index in the diff\n",
        "            'original_position': c.get('original_position'),\n",
        "            'commit_id': c.get('commit_id'), # SHA of commit comment refers to\n",
        "            'original_commit_id': c.get('original_commit_id'),\n",
        "            'created_at': c.get('created_at'),\n",
        "            'updated_at': c.get('updated_at'),\n",
        "            'in_reply_to_id': c.get('in_reply_to_id') # For comment threads\n",
        "        } for c in comments if isinstance(c, dict) # Ensure item is a dictionary\n",
        "    ]\n",
        "\n",
        "def get_pr_issue_comments(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all general issue comments (comments on the PR itself) for a PR. Returns a list of simplified comment dictionaries.\"\"\"\n",
        "    print(f\"    Fetching issue comments for PR #{pr_number}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/issues/{pr_number}/comments\"\n",
        "    comments = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not comments: return []\n",
        "    return [\n",
        "        {\n",
        "            'id': c.get('id'),\n",
        "            'user': c.get('user', {}).get('login', 'ghost'), # Use .get for nested access safety\n",
        "            'body': c.get('body'), # Full body\n",
        "            'created_at': c.get('created_at'),\n",
        "            'updated_at': c.get('updated_at')\n",
        "        } for c in comments if isinstance(c, dict) # Ensure item is a dictionary\n",
        "    ]\n",
        "\n",
        "def get_pr_commits(owner, repo, pr_number, headers):\n",
        "    \"\"\"Fetches all commits associated with a PR. Returns a list of simplified commit dictionaries.\"\"\"\n",
        "    print(f\"    Fetching commits for PR #{pr_number}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/commits\"\n",
        "    commits = fetch_paginated_data(api_url, headers=headers)\n",
        "    if not commits: return []\n",
        "    return [\n",
        "        {\n",
        "            'sha': c.get('sha'),\n",
        "            'message': c.get('commit', {}).get('message'), # Use .get for nested access safety\n",
        "            'author': c.get('commit', {}).get('author'), # { name, email, date } - Use .get for safety\n",
        "            'committer': c.get('commit', {}).get('committer'), # { name, email, date } - Use .get for safety\n",
        "            'api_author_login': c.get('author', {}).get('login') if c.get('author') else None, # Use .get for safety\n",
        "            'api_committer_login': c.get('committer', {}).get('login') if c.get('committer') else None, # Use .get for safety\n",
        "            'parents': [p.get('sha') for p in c.get('parents', []) if isinstance(p, dict) and p.get('sha')] # Safely get parent SHAs\n",
        "        } for c in commits if isinstance(c, dict) # Ensure item is a dictionary\n",
        "    ]\n",
        "\n",
        "def get_commit_check_runs(owner, repo, ref_sha, headers):\n",
        "    \"\"\"Fetches check runs (newer Checks API) for a specific commit SHA. Returns a list of simplified check run dictionaries.\"\"\"\n",
        "    if not ref_sha: return []\n",
        "    print(f\"    Fetching check runs for commit {ref_sha[:7]}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{ref_sha}/check-runs\"\n",
        "    # Check runs API might return an object containing the list.\n",
        "    params={'per_page': 100} # Request max per page\n",
        "    response = make_api_request(api_url, headers=headers, params=params)\n",
        "\n",
        "    if response and response.status_code == 200:\n",
        "        try:\n",
        "            data = response.json()\n",
        "            check_runs_list = []\n",
        "            # The API returns an object with a 'check_runs' key\n",
        "            if isinstance(data, dict) and 'check_runs' in data and isinstance(data['check_runs'], list):\n",
        "                 check_runs_list = data['check_runs']\n",
        "                 # Note: Check runs API pagination is different, usually handled via 'link' header on the main check-suites endpoint\n",
        "                 # This simple fetch gets the first page of check runs for a commit.\n",
        "            elif isinstance(data, list): # Fallback if it unexpectedly returns a list directly\n",
        "                 check_runs_list = data\n",
        "            else:\n",
        "                 print(f\"    Unexpected response format for check runs for commit {ref_sha[:7]}. Response type: {type(data)}. Keys: {data.keys() if isinstance(data, dict) else 'N/A'}\")\n",
        "                 return []\n",
        "\n",
        "            return [\n",
        "                {\n",
        "                    'name': cr.get('name'),\n",
        "                    'status': cr.get('status'), # e.g., 'queued', 'in_progress', 'completed'\n",
        "                    'conclusion': cr.get('conclusion'), # e.g., 'success', 'failure', 'neutral', 'cancelled', 'skipped', 'timed_out', 'action_required'\n",
        "                    'started_at': cr.get('started_at'),\n",
        "                    'completed_at': cr.get('completed_at'),\n",
        "                    'app_owner': cr.get('app', {}).get('owner', {}).get('login') if isinstance(cr.get('app'), dict) else None, # Use .get for nested access safety\n",
        "                    'app_name': cr.get('app', {}).get('name') if isinstance(cr.get('app'), dict) else None # Use .get for nested access safety\n",
        "                    # Could add 'output' field (summary, text, annotations) but it can be large\n",
        "                } for cr in check_runs_list if isinstance(cr, dict) # Ensure item is a dictionary\n",
        "            ]\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"    Error decoding JSON for check runs commit {ref_sha[:7]}: {e}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "             print(f\"    An unexpected error occurred processing check runs for commit {ref_sha[:7]}: {e}\")\n",
        "             return []\n",
        "    return [] # Return empty list on error or no checks found\n",
        "\n",
        "def get_commit_statuses(owner, repo, ref_sha, headers):\n",
        "    \"\"\"Fetches statuses (older Status API) for a specific commit SHA. Returns a list of simplified status dictionaries.\"\"\"\n",
        "    if not ref_sha: return []\n",
        "    print(f\"    Fetching statuses for commit {ref_sha[:7]}...\")\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{ref_sha}/statuses\"\n",
        "    # Statuses are usually returned as a direct list and paginated\n",
        "    statuses = fetch_paginated_data(api_url, headers=headers) # Use paginated fetch\n",
        "    if not statuses: return []\n",
        "    return [\n",
        "        {\n",
        "            'context': s.get('context'), # Name of the status check\n",
        "            'state': s.get('state'), # e.g., 'error', 'failure', 'pending', 'success'\n",
        "            'description': s.get('description'), # Short description\n",
        "            'target_url': s.get('target_url'), # Link to the status details\n",
        "            'creator_login': s.get('creator', {}).get('login') if isinstance(s.get('creator'), dict) else None, # Use .get for nested access safety\n",
        "            'created_at': s.get('created_at'),\n",
        "            'updated_at': s.get('updated_at')\n",
        "        } for s in statuses if isinstance(s, dict) # Ensure item is a dictionary\n",
        "    ]\n",
        "\n",
        "def parse_linked_issues(text):\n",
        "    \"\"\"\n",
        "    Rudimentary parsing for linked issue references (GitHub, Jira style) in text.\n",
        "    Returns a sorted list of unique issue references found.\n",
        "    Args:\n",
        "        text (str): The text to parse for issue references.\n",
        "    Returns:\n",
        "        list: A sorted list of unique issue reference strings.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    # GitHub: Keywords like close/fix/resolve followed by #123\n",
        "    github_refs_keyword = re.findall(r'(?:close(?:s|d)?|resolve(?:s|d)?|fix(?:es|ed)?)\\s+#(\\d+)', text, re.IGNORECASE)\n",
        "    # GitHub: Simple references like #123 (ensure it's not part of a word)\n",
        "    github_refs_simple = re.findall(r'(?<![a-zA-Z0-9])#(\\d+)\\b', text)\n",
        "    # Jira: Common pattern like PROJECT-123\n",
        "    jira_refs = re.findall(r'\\b([A-Z][A-Z0-9_]+-\\d+)\\b', text)\n",
        "\n",
        "    issues = set()\n",
        "    # Add GitHub refs with a prefix for clarity\n",
        "    for ref in github_refs_keyword: issues.add(f\"GH-{ref}\")\n",
        "    # Add simple refs only if not already captured by keyword refs\n",
        "    for ref in github_refs_simple:\n",
        "        if f\"GH-{ref}\" not in issues:\n",
        "             issues.add(f\"GH-{ref}\")\n",
        "    # Add Jira refs\n",
        "    for ref in jira_refs: issues.add(ref)\n",
        "\n",
        "    return sorted(list(issues))\n",
        "\n",
        "# --- Main Data Fetching Function ---\n",
        "\n",
        "def get_all_pull_requests_structured(owner, repo, state='all'):\n",
        "    \"\"\"\n",
        "    Get ALL pull requests for a repo, fetching detailed information and saving\n",
        "    details into a structured folder format for each PR.\n",
        "\n",
        "    WARNING: Can be very time-consuming and disk-space intensive for large repos.\n",
        "             Highly likely to hit rate limits without appropriate delays.\n",
        "    Args:\n",
        "        owner (str): Repository owner.\n",
        "        repo (str): Repository name.\n",
        "        state (str, optional): State of the pull requests ('open', 'closed', 'all'). Defaults to 'all'.\n",
        "    Returns:\n",
        "        list: A list of PR numbers for which data was successfully processed.\n",
        "    \"\"\"\n",
        "    if not github_secret:\n",
        "         print(\"Skipping data fetching: GITHUB_BOT_ACCESS_TOKEN is not set.\")\n",
        "         return []\n",
        "\n",
        "    print(f\"--- Starting STRUCTURED pull request data fetch for {owner}/{repo} ---\")\n",
        "    print(f\"--- Output base directory: {OUTPUT_DIR_BASE} ---\")\n",
        "    # print(f\"--- WARNING: Rate limit delays are currently DISABLED. Monitor API usage. ---\") # Keep if delays are off\n",
        "\n",
        "    os.makedirs(OUTPUT_DIR_BASE, exist_ok=True) # Ensure base output directory exists\n",
        "\n",
        "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls\"\n",
        "    headers = {\n",
        "        'Authorization': f'token {github_secret}',\n",
        "        'Accept': 'application/vnd.github.v3+json', # Base API v3 format\n",
        "    }\n",
        "    params = {\n",
        "        'state': state,        # 'open', 'closed', 'all'\n",
        "        'per_page': PER_PAGE,\n",
        "        'sort': 'updated',     # 'created', 'updated', 'popularity', 'long-running'\n",
        "        'direction': 'desc',   # Get most recently updated first\n",
        "        'page': 1              # Initial page number\n",
        "    }\n",
        "    # No date filtering applied in this version\n",
        "\n",
        "    processed_pr_numbers = [] # Keep track of processed PRs\n",
        "    current_url = api_url\n",
        "    page = 1\n",
        "\n",
        "    while current_url:\n",
        "        # Ensure the URL format is correct (remove potential brackets)\n",
        "        cleaned_url = current_url.replace('[https://', 'https://').replace(']', '')\n",
        "        print(f\"\\nFetching page {page} of pull requests list from {cleaned_url.split('?')[0]}...\")\n",
        "\n",
        "        # Use initial params only on the very first request URL if it doesn't have query string\n",
        "        # Otherwise, the 'next' URL from headers includes all necessary parameters\n",
        "        page_params = params if page == 1 and '?' not in cleaned_url else None\n",
        "\n",
        "        response = make_api_request(cleaned_url, headers=headers, params=page_params)\n",
        "        page += 1\n",
        "\n",
        "        if not response:\n",
        "            print(\"Failed to fetch pull requests list page. Stopping.\")\n",
        "            break # Exit if API request fails (could be rate limit, network issue, etc.)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                pull_requests_page = response.json()\n",
        "                if not pull_requests_page or not isinstance(pull_requests_page, list):\n",
        "                    print(\"No more pull requests found on this page or unexpected format.\")\n",
        "                    break # Stop if page is empty or not a list\n",
        "\n",
        "                print(f\"Processing {len(pull_requests_page)} pull requests from page {page-1}...\")\n",
        "\n",
        "                for pr_summary in pull_requests_page:\n",
        "                    # Ensure pr_summary is a dictionary before processing\n",
        "                    if not isinstance(pr_summary, dict):\n",
        "                        print(f\"Skipping unexpected item in PR list (not a dictionary): {pr_summary}\")\n",
        "                        continue\n",
        "\n",
        "                    pr_number = pr_summary.get('number')\n",
        "                    if pr_number is None:\n",
        "                         print(f\"Skipping PR summary with missing number: {pr_summary}\")\n",
        "                         continue\n",
        "\n",
        "                    pr_updated_at = pr_summary.get('updated_at')\n",
        "                    print(f\"\\n--- Processing PR #{pr_number}: {pr_summary.get('title', 'N/A')} ---\")\n",
        "                    print(f\"    Updated at: {pr_updated_at}\")\n",
        "\n",
        "                    # --- Create PR Specific Directory ---\n",
        "                    pr_output_dir = os.path.join(OUTPUT_DIR_BASE, f\"pr_{pr_number}\")\n",
        "                    os.makedirs(pr_output_dir, exist_ok=True)\n",
        "\n",
        "                    # --- Basic PR Info ---\n",
        "                    # It's often better to fetch the full PR details even if the list provides some\n",
        "                    # as the full endpoint might have more/updated info.\n",
        "                    print(f\"    Fetching full details for PR #{pr_number}...\")\n",
        "                    pr_detail_url = pr_summary.get('url') # URL for the specific PR\n",
        "                    if not pr_detail_url:\n",
        "                        print(f\"    ERROR: Missing 'url' in PR summary for #{pr_number}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    pr_detail_response = make_api_request(pr_detail_url, headers=headers)\n",
        "                    if not pr_detail_response or pr_detail_response.status_code != 200:\n",
        "                         print(f\"    ERROR: Failed to fetch full details for PR #{pr_number}. Skipping.\")\n",
        "                         continue\n",
        "\n",
        "                    try:\n",
        "                        pr = pr_detail_response.json() # Full PR data\n",
        "                        if not isinstance(pr, dict):\n",
        "                             print(f\"    ERROR: Full PR details for #{pr_number} is not a dictionary. Skipping.\")\n",
        "                             continue\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"    ERROR: Failed to decode JSON for full PR #{pr_number} details: {e}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Extract key details needed for subsequent calls\n",
        "                    base_sha = pr.get('base', {}).get('sha') # Use .get for nested access safety\n",
        "                    head_sha = pr.get('head', {}).get('sha') # Use .get for nested access safety\n",
        "                    pr_body = pr.get('body') # Full body\n",
        "\n",
        "                    if not base_sha or not head_sha:\n",
        "                         print(f\"    Warning: Missing base_sha ('{base_sha}') or head_sha ('{head_sha}') for PR #{pr_number}. File content fetching might be incomplete.\")\n",
        "                         # Decide if you want to skip or continue with partial data\n",
        "                         # continue # Uncomment to skip PRs with missing SHAs\n",
        "\n",
        "                    # --- Fetch and Save Files (Content & Patches) ---\n",
        "                    # This function now handles saving files and returns metadata list\n",
        "                    files_metadata = process_pull_request_files(\n",
        "                        owner, repo, pr_number, base_sha, head_sha, headers, pr_output_dir\n",
        "                    )\n",
        "\n",
        "                    # --- Fetch Other Details (Comments, Commits, Reviews, Checks) ---\n",
        "                    reviews = get_pr_reviews(owner, repo, pr_number, headers)\n",
        "                    review_comments = get_pr_review_comments(owner, repo, pr_number, headers)\n",
        "                    issue_comments = get_pr_issue_comments(owner, repo, pr_number, headers)\n",
        "                    commits_list = get_pr_commits(owner, repo, pr_number, headers)\n",
        "\n",
        "                    check_runs = []\n",
        "                    statuses = []\n",
        "                    if head_sha:\n",
        "                        check_runs = get_commit_check_runs(owner, repo, head_sha, headers)\n",
        "                        # Optionally fetch statuses only if check runs are empty or always fetch both\n",
        "                        statuses = get_commit_statuses(owner, repo, head_sha, headers)\n",
        "                    else:\n",
        "                        print(f\"    Skipping checks/statuses fetch for PR #{pr_number} due to missing head_sha.\")\n",
        "\n",
        "                    # --- Parse Linked Issues ---\n",
        "                    linked_issues = set()\n",
        "                    if pr_body: # Check if body exists before parsing\n",
        "                        linked_issues.update(parse_linked_issues(pr_body))\n",
        "                    for c in commits_list:\n",
        "                        # Ensure commit message exists before parsing\n",
        "                        if isinstance(c, dict) and c.get('message'):\n",
        "                            linked_issues.update(parse_linked_issues(c.get('message')))\n",
        "                    # Also parse issue comments and review bodies/comments for links\n",
        "                    for ic in issue_comments:\n",
        "                        if isinstance(ic, dict) and ic.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(ic.get('body')))\n",
        "                    for r in reviews:\n",
        "                         if isinstance(r, dict) and r.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(r.get('body')))\n",
        "                    for rc in review_comments:\n",
        "                         if isinstance(rc, dict) and rc.get('body'):\n",
        "                            linked_issues.update(parse_linked_issues(rc.get('body')))\n",
        "\n",
        "\n",
        "                    # --- Assemble Metadata (excluding file content/patches) ---\n",
        "                    metadata = {\n",
        "                        'pr_number': pr_number,\n",
        "                        'api_url': pr.get('url'),\n",
        "                        'html_url': pr.get('html_url'),\n",
        "                        'state': pr.get('state'),\n",
        "                        'title': pr.get('title'),\n",
        "                        'author_login': pr.get('user', {}).get('login', 'ghost') if isinstance(pr.get('user'), dict) else 'ghost', # Use .get for nested access safety\n",
        "                        'author_association': pr.get('author_association'),\n",
        "                        'body': pr_body, # Full body text\n",
        "                        'created_at': pr.get('created_at'),\n",
        "                        'updated_at': pr.get('updated_at'),\n",
        "                        'closed_at': pr.get('closed_at'),\n",
        "                        'merged_at': pr.get('merged_at'),\n",
        "                        'merge_commit_sha': pr.get('merge_commit_sha'),\n",
        "                        'assignee': pr.get('assignee', {}).get('login') if isinstance(pr.get('assignee'), dict) else None, # Use .get for nested access safety\n",
        "                        'assignees': [a.get('login') for a in pr.get('assignees', []) if isinstance(a, dict) and a.get('login')], # Safely get assignees\n",
        "                        'requested_reviewers': [rr.get('login') for rr in pr.get('requested_reviewers', []) if isinstance(rr, dict) and rr.get('login')], # Safely get reviewers\n",
        "                        'requested_teams': [rt.get('slug') for rt in pr.get('requested_teams', []) if isinstance(rt, dict) and rt.get('slug')], # Safely get teams\n",
        "                        'labels': [l.get('name') for l in pr.get('labels', []) if isinstance(l, dict) and l.get('name')], # Safely get label names\n",
        "                        'is_draft': pr.get('draft', False),\n",
        "                        'merged': pr.get('merged', False),\n",
        "                        'mergeable': pr.get('mergeable'), # Note: Can be None if GitHub hasn't calculated it yet\n",
        "                        'mergeable_state': pr.get('mergeable_state'), # e.g., 'clean', 'dirty', 'unknown'\n",
        "                        'merged_by_login': pr.get('merged_by', {}).get('login') if isinstance(pr.get('merged_by'), dict) else None, # Use .get for nested access safety\n",
        "                        'base_branch': pr.get('base', {}).get('ref') if isinstance(pr.get('base'), dict) else None, # Use .get for nested access safety\n",
        "                        'base_commit_sha': base_sha,\n",
        "                        'head_branch': pr.get('head', {}).get('ref') if isinstance(pr.get('head'), dict) else None, # Use .get for nested access safety\n",
        "                        'head_repo_full_name': pr.get('head', {}).get('repo', {}).get('full_name') if isinstance(pr.get('head'), dict) and isinstance(pr.get('head').get('repo'), dict) else None, # Use .get for nested access safety\n",
        "                        'head_commit_sha': head_sha,\n",
        "                        'reviews': reviews,\n",
        "                        'review_comments': review_comments,\n",
        "                        'issue_comments': issue_comments,\n",
        "                        'commits_list': commits_list,\n",
        "                        'commits_count': len(commits_list), # Use length of fetched list\n",
        "                        'check_runs': check_runs, # Check runs associated with the head commit\n",
        "                        'statuses': statuses,     # Statuses associated with the head commit\n",
        "                        'linked_issues_parsed': sorted(list(linked_issues)),\n",
        "                        'changed_files_count': len(files_metadata), # Use length of processed files list\n",
        "                        'total_additions': sum(f.get('additions', 0) for f in files_metadata), # Sum from metadata\n",
        "                        'total_deletions': sum(f.get('deletions', 0) for f in files_metadata), # Sum from metadata\n",
        "                        'changed_files_manifest': files_metadata # List of processed files (metadata only)\n",
        "                    }\n",
        "\n",
        "                    # --- Save Metadata to JSON file ---\n",
        "                    metadata_filename = os.path.join(pr_output_dir, \"metadata.json\")\n",
        "                    try:\n",
        "                        with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "                            # Use indent for readability, None for smallest size\n",
        "                            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "                        print(f\"    Successfully saved metadata to {metadata_filename}\")\n",
        "                        processed_pr_numbers.append(pr_number)\n",
        "                    except IOError as e:\n",
        "                        print(f\"    Error writing metadata JSON file {metadata_filename}: {e}\")\n",
        "                    except TypeError as e:\n",
        "                        print(f\"    Error serializing metadata JSON for PR #{pr_number}: {e}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Unexpected error saving metadata JSON for PR #{pr_number}: {e}\")\n",
        "\n",
        "                # --- Check for next page link ---\n",
        "                if 'next' in response.links:\n",
        "                    current_url = response.links['next']['url']\n",
        "                    params = None # Params are included in the 'next' URL for subsequent pages\n",
        "                    print(f\"--- Moving to next page of PR list ---\")\n",
        "                else:\n",
        "                    print(\"No 'next' link found in PR list response, reached the last page.\")\n",
        "                    current_url = None # End the loop\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON from PR list page {page-1}: {e}\")\n",
        "                print(f\"Response text: {response.text[:200]}...\") # Print start of text for debugging\n",
        "                break # Stop pagination on JSON error\n",
        "            except Exception as e:\n",
        "                 print(f\"An unexpected error occurred while processing PRs on page {page-1}: {e}\")\n",
        "                 break # Safer to stop if unexpected errors occur\n",
        "\n",
        "        else: # Handle non-200 status for the PR list page\n",
        "            print(f\"Stopping pagination. Received status {response.status_code} for PR list page {page-1}.\")\n",
        "            break # Exit loop if fetching the list of PRs fails\n",
        "\n",
        "    print(f\"\\n--- Finished STRUCTURED processing for {owner}/{repo}. ---\")\n",
        "    print(f\"--- Processed {len(processed_pr_numbers)} pull requests. ---\")\n",
        "    print(f\"--- Data saved in subdirectories within: {OUTPUT_DIR_BASE} ---\")\n",
        "    return processed_pr_numbers # Return list of processed PR numbers\n",
        "\n",
        "\n",
        "# --- RAG System ---\n",
        "class PRSpecificRAG:\n",
        "    \"\"\"\n",
        "    A Retrieval-Augmented Generation system for analyzing Pull Request data.\n",
        "    Uses Chroma as the vector store and a HuggingFace model for the LLM.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path=\"pull_request_data_structured\"):\n",
        "        \"\"\"\n",
        "        Initializes the RAG system with data path, embeddings, and text splitter.\n",
        "        Args:\n",
        "            data_path (str, optional): The base directory containing structured PR data.\n",
        "                                       Defaults to \"pull_request_data_structured\".\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        # Initialize embeddings model (CodeBERT for code understanding)\n",
        "        print(\"Initializing embeddings model...\")\n",
        "        try:\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"microsoft/graphcodebert-base\",\n",
        "                model_kwargs={\"trust_remote_code\": True} # Needed for some models\n",
        "            )\n",
        "            print(\"Embeddings model initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing embeddings model: {str(e)}\")\n",
        "            self.embeddings = None # Ensure embeddings is None if initialization fails\n",
        "            # Depending on criticality, you might want to raise an exception here\n",
        "            # raise e\n",
        "\n",
        "\n",
        "        # Initialize text splitter for Python code\n",
        "        self.splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "            language=Language.PYTHON,\n",
        "            chunk_size=2048, # Size of text chunks\n",
        "            chunk_overlap=50 # Overlap between chunks to maintain context\n",
        "        )\n",
        "        self.pr_databases = {} # Dictionary to store Chroma DBs for each PR (in-memory)\n",
        "        self.llm = None # LLM will be initialized separately\n",
        "\n",
        "        # Keep track of temporary directories created for Chroma\n",
        "        self._temp_chroma_dirs = {}\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"\n",
        "        Destructor to clean up temporary Chroma directories when the object is deleted.\n",
        "        \"\"\"\n",
        "        print(\"Cleaning up temporary Chroma directories...\")\n",
        "        for pr_number, temp_dir in self._temp_chroma_dirs.items():\n",
        "            try:\n",
        "                if os.path.exists(temp_dir):\n",
        "                    shutil.rmtree(temp_dir)\n",
        "                    print(f\"Cleaned up temporary directory for PR {pr_number}: {temp_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error cleaning up temporary directory {temp_dir} for PR {pr_number}: {e}\")\n",
        "\n",
        "\n",
        "    def _load_pr_metadata(self, pr_dir):\n",
        "        \"\"\"\n",
        "        Loads the metadata.json file for a single PR directory.\n",
        "        Returns the metadata dictionary.\n",
        "        Args:\n",
        "            pr_dir (str): The path to the specific PR directory.\n",
        "        Returns:\n",
        "            dict: The metadata dictionary.\n",
        "        Raises:\n",
        "            FileNotFoundError: If metadata file is not found.\n",
        "            json.JSONDecodeError: If metadata file is invalid JSON.\n",
        "        \"\"\"\n",
        "        metadata_path = os.path.join(pr_dir, \"metadata.json\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(f\"Metadata file not found for PR in {pr_dir}\")\n",
        "        with open(metadata_path, \"r\", encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def _process_single_pr(self, pr_dir_name):\n",
        "        \"\"\"\n",
        "        Processes data for a single PR directory, creates text chunks,\n",
        "        and builds a Chroma vector database using a temporary directory for persistence.\n",
        "        Args:\n",
        "            pr_dir_name (str): The name of the specific PR directory (e.g., \"pr_123\").\n",
        "        Returns:\n",
        "            Chroma: The Chroma vector database instance, or None on failure.\n",
        "        \"\"\"\n",
        "        pr_number_str = pr_dir_name.split(\"_\")[-1] # Get PR number from directory name\n",
        "        try:\n",
        "            pr_number = int(pr_number_str)\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not parse PR number from directory name: {pr_dir_name}. Skipping.\")\n",
        "            return None # Skip if PR number cannot be parsed\n",
        "\n",
        "        full_path = os.path.join(self.data_path, pr_dir_name)\n",
        "        if not os.path.isdir(full_path):\n",
        "             print(f\"Warning: PR directory not found or is not a directory: {full_path}. Skipping.\")\n",
        "             return None # Skip if directory doesn't exist\n",
        "\n",
        "        print(f\"Processing data for PR #{pr_number_str}...\") # Use string for logging\n",
        "\n",
        "        try:\n",
        "            # Load metadata even if not explicitly stored in Chroma, as it's used for context creation\n",
        "            metadata = self._load_pr_metadata(full_path)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading metadata for PR #{pr_number_str}: {e}. Skipping.\")\n",
        "            return None\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error decoding metadata JSON for PR #{pr_number_str}: {e}. Skipping.\")\n",
        "             return None\n",
        "        except Exception as e:\n",
        "             print(f\"An unexpected error occurred loading metadata for PR #{pr_number_str}: {e}. Skipping.\")\n",
        "             return None\n",
        "\n",
        "        if self.embeddings is None:\n",
        "             print(f\"Skipping vector DB creation for PR #{pr_number_str}: Embeddings model not initialized.\")\n",
        "             return None\n",
        "\n",
        "        chunks = []\n",
        "        # Process changed files\n",
        "        changed_files = metadata.get(\"changed_files_manifest\", [])\n",
        "        if not changed_files:\n",
        "             print(f\"No changed files found in metadata for PR #{pr_number_str}.\")\n",
        "             pass # Continue to process other data\n",
        "\n",
        "        for file_meta in tqdm(changed_files, desc=f\"Processing files for PR #{pr_number_str}\"):\n",
        "            # Ensure file_meta is a dictionary and has a filename\n",
        "            if not isinstance(file_meta, dict) or 'filename' not in file_meta:\n",
        "                 print(f\"Warning: Skipping invalid file metadata entry for PR #{pr_number_str}: {file_meta}\")\n",
        "                 continue\n",
        "\n",
        "            filename = file_meta[\"filename\"]\n",
        "            try:\n",
        "                # Load code and patch content\n",
        "                before_code = self._read_code_file(full_path, \"before_merge\", filename)\n",
        "                after_code = self._read_code_file(full_path, \"after_merge\", filename)\n",
        "                patch = self._read_patch_file(full_path, filename)\n",
        "\n",
        "                # Create context string for the file\n",
        "                context = self._create_context(metadata, filename, before_code, after_code, patch)\n",
        "\n",
        "                # Split the context into smaller chunks\n",
        "                file_chunks = self.splitter.split_text(context)\n",
        "\n",
        "                # Add chunks.\n",
        "                chunks.extend(file_chunks)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename} in PR {pr_number_str}: {str(e)}\")\n",
        "\n",
        "        # Optionally, add PR body and comments as separate documents if needed\n",
        "        # This data will also be chunked and added to Chroma\n",
        "        pr_body = metadata.get(\"body\")\n",
        "        if pr_body:\n",
        "             body_chunks = self.splitter.split_text(f\"PR Body:\\n{pr_body}\")\n",
        "             chunks.extend(body_chunks)\n",
        "\n",
        "\n",
        "        issue_comments = metadata.get(\"issue_comments\", [])\n",
        "        for comment in issue_comments:\n",
        "             if isinstance(comment, dict) and comment.get(\"body\"):\n",
        "                  comment_chunks = self.splitter.split_text(f\"Issue Comment by {comment.get('user', 'N/A')}:\\n{comment.get('body')}\")\n",
        "                  chunks.extend(comment_chunks)\n",
        "\n",
        "\n",
        "        review_comments = metadata.get(\"review_comments\", [])\n",
        "        for comment in review_comments:\n",
        "             if isinstance(comment, dict) and comment.get(\"body\"):\n",
        "                  comment_chunks = self.splitter.split_text(f\"Review Comment by {comment.get('user', 'N/A')} on {comment.get('path', 'N/A')}:\\n{comment.get('body')}\")\n",
        "                  chunks.extend(comment_chunks)\n",
        "\n",
        "\n",
        "        if not chunks:\n",
        "            print(f\"No processable content found for PR #{pr_number_str}. Skipping vector DB creation.\")\n",
        "            return None # Skip if no chunks were created\n",
        "\n",
        "        # Create a temporary directory for Chroma persistence for this PR\n",
        "        # This helps avoid conflicts and ensures a clean DB each time.\n",
        "        try:\n",
        "            temp_dir = tempfile.mkdtemp(prefix=f\"chroma_db_pr_{pr_number_str}_\")\n",
        "            self._temp_chroma_dirs[pr_number_str] = temp_dir # Store for cleanup\n",
        "\n",
        "            vector_db = Chroma.from_texts(\n",
        "                texts=chunks,\n",
        "                embedding=self.embeddings,\n",
        "                persist_directory=temp_dir # Explicitly use the temporary directory\n",
        "            )\n",
        "            print(f\"Successfully created Chroma DB for PR #{pr_number_str} in temporary directory: {temp_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Chroma DB for PR #{pr_number_str}: {str(e)}\")\n",
        "            # Clean up the temporary directory if creation failed\n",
        "            if pr_number_str in self._temp_chroma_dirs:\n",
        "                 try:\n",
        "                     shutil.rmtree(self._temp_chroma_dirs[pr_number_str])\n",
        "                     del self._temp_chroma_dirs[pr_number_str]\n",
        "                 except Exception as cleanup_e:\n",
        "                     print(f\"Error during cleanup of temp dir {temp_dir}: {cleanup_e}\")\n",
        "\n",
        "            return None # Return None if DB creation fails\n",
        "\n",
        "        # Store the created vector database in the dictionary\n",
        "        self.pr_databases[pr_number_str] = vector_db # Store with string key\n",
        "        return vector_db\n",
        "\n",
        "    def _read_code_file(self, pr_path, dir_name, filename):\n",
        "        \"\"\"\n",
        "        Reads content from a code file within a PR directory.\n",
        "        Returns the file content as a string or an empty string if not found/error.\n",
        "        Args:\n",
        "            pr_path (str): The full path to the specific PR directory.\n",
        "            dir_name (str): The subdirectory name ('before_merge' or 'after_merge').\n",
        "            filename (str): The name of the file.\n",
        "        Returns:\n",
        "            str: The file content as a string, or empty string if not found/error.\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(pr_path, dir_name, filename)\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
        "                return \"\" # Return empty string on error\n",
        "        return \"\" # Return empty string if file doesn't exist\n",
        "\n",
        "    def _read_patch_file(self, pr_path, filename):\n",
        "        \"\"\"\n",
        "        Reads content from a patch file within a PR directory.\n",
        "        Returns the patch content as a string or an empty string if not found/error.\n",
        "         Args:\n",
        "            pr_path (str): The full path to the specific PR directory.\n",
        "            filename (str): The name of the original file (used to construct patch filename).\n",
        "        Returns:\n",
        "            str: The patch content as a string, or empty string if not found/error.\n",
        "        \"\"\"\n",
        "        patch_path = os.path.join(pr_path, \"changed_files\", filename + \".patch\")\n",
        "        if os.path.exists(patch_path):\n",
        "            try:\n",
        "                with open(patch_path, \"r\", encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading patch file {patch_path}: {str(e)}\")\n",
        "                return \"\" # Return empty string on error\n",
        "        return \"\" # Return empty string if file doesn't exist\n",
        "\n",
        "    def _create_context(self, metadata, filename, before_code, after_code, patch):\n",
        "        \"\"\"\n",
        "        Creates a combined context string for a specific file within a PR,\n",
        "        including PR details, code changes, and relevant comments.\n",
        "        Args:\n",
        "            metadata (dict): The full PR metadata dictionary.\n",
        "            filename (str): The name of the file being processed.\n",
        "            before_code (str): The code content before the change.\n",
        "            after_code (str): The code content after the change.\n",
        "            patch (str): The patch/diff content.\n",
        "        Returns:\n",
        "            str: A formatted string containing the context for the RAG model.\n",
        "        \"\"\"\n",
        "        # Filter review comments relevant to this specific file\n",
        "        file_review_comments = [\n",
        "            c.get('body') for c in metadata.get('review_comments', [])\n",
        "            if isinstance(c, dict) and c.get('path') == filename and c.get('body') # Ensure valid comment dict and body/path exist\n",
        "        ]\n",
        "        comments_text = \"\\n\".join(file_review_comments) if file_review_comments else \"No specific review comments for this file.\"\n",
        "\n",
        "        # Safely get CI check names\n",
        "        ci_checks_list = [c.get('name', 'N/A') for c in metadata.get('check_runs', []) if isinstance(c, dict)]\n",
        "        ci_checks_str = \", \".join(ci_checks_list) if ci_checks_list else \"No CI checks found.\"\n",
        "\n",
        "\n",
        "        context = (\n",
        "            f\"--- Pull Request #{metadata.get('pr_number', 'N/A')} - {metadata.get('title', 'N/A')} ---\\n\"\n",
        "            f\"Author: {metadata.get('author_login', 'N/A')}\\n\"\n",
        "            f\"File: {filename}\\n\"\n",
        "            f\"Status: {metadata.get('state', 'N/A')}\\n\"\n",
        "            f\"CI Checks for head commit: {ci_checks_str}\\n\\n\"\n",
        "            f\"BEFORE CODE:\\n{before_code}\\n\\n\"\n",
        "            f\"AFTER CODE:\\n{after_code}\\n\\n\"\n",
        "            f\"DIFF:\\n{patch}\\n\\n\"\n",
        "            f\"REVIEW COMMENTS on this file:\\n{comments_text}\\n\"\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def initialize_llm(self):\n",
        "        \"\"\"\n",
        "        Initializes the HuggingFace Language Model pipeline.\n",
        "        Loads the tokenizer and model, sets up the text generation pipeline.\n",
        "        \"\"\"\n",
        "        if self.llm is not None:\n",
        "            print(\"LLM already initialized.\")\n",
        "            return\n",
        "\n",
        "        # model_name = \"HuggingFaceH4/zephyr-7b-beta\" # Larger model example\n",
        "        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Smaller model for faster local testing\n",
        "        print(f\"Initializing LLM: {model_name}...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "            text_gen_pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                temperature=0.1, # Controls randomness (lower is more deterministic)\n",
        "                max_new_tokens=512, # Max tokens to generate in the response\n",
        "                repetition_penalty=1.1 # Penalize repeating tokens\n",
        "                # Add other pipeline arguments as needed (e.g., device='cuda' or device='cpu')\n",
        "            )\n",
        "\n",
        "            self.llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
        "            print(\"LLM initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing LLM: {str(e)}\")\n",
        "            self.llm = None # Ensure LLM is None if initialization fails\n",
        "            # Depending on criticality, you might want to raise an exception here\n",
        "            # raise e\n",
        "\n",
        "\n",
        "    def load_pr(self, pr_number):\n",
        "        \"\"\"\n",
        "        Loads data for a specific PR number into an in-memory vector database.\n",
        "        Processes the raw data files saved by get_all_pull_requests_structured.\n",
        "        Returns the created Chroma DB instance or None on failure.\n",
        "        Args:\n",
        "            pr_number (int or str): The pull request number.\n",
        "        Returns:\n",
        "            Chroma: The Chroma vector database instance, or None on failure.\n",
        "        \"\"\"\n",
        "        pr_number_str = str(pr_number) # Ensure key is string\n",
        "        pr_dir_name = f\"pr_{pr_number_str}\" # Ensure directory name uses string PR number\n",
        "        pr_dir_path = os.path.join(self.data_path, pr_dir_name)\n",
        "\n",
        "        if not os.path.exists(pr_dir_path):\n",
        "            print(f\"Error: Data directory for PR #{pr_number_str} not found at {pr_dir_path}\")\n",
        "            return None\n",
        "\n",
        "        # Process the PR data and create/store the vector DB\n",
        "        # _process_single_pr now handles creating/storing the DB and using a temp dir\n",
        "        vector_db = self._process_single_pr(pr_dir_name)\n",
        "        return vector_db # Returns the DB instance or None\n",
        "\n",
        "    def get_review(self, pr_number, question):\n",
        "        \"\"\"\n",
        "        Gets a review/answer for a specific question about a PR.\n",
        "        Loads PR data if necessary, retrieves relevant context, formats the prompt,\n",
        "        and invokes the LLM directly.\n",
        "        Args:\n",
        "            pr_number (int or str): The pull request number.\n",
        "            question (str): The question to ask about the PR.\n",
        "        Returns:\n",
        "            dict: A dictionary containing the PR number, question, answer, and sources.\n",
        "        \"\"\"\n",
        "        pr_number_str = str(pr_number) # Ensure key is string\n",
        "\n",
        "        try:\n",
        "            if self.llm is None:\n",
        "                 raise ValueError(\"LLM is not initialized. Call initialize_llm() first.\")\n",
        "\n",
        "            # Load PR data if not already in memory or if the stored DB is None\n",
        "            if pr_number_str not in self.pr_databases or self.pr_databases[pr_number_str] is None:\n",
        "                print(f\"PR #{pr_number_str} data not loaded. Attempting to load...\")\n",
        "                loaded_db = self.load_pr(pr_number_str) # Pass as string\n",
        "                if loaded_db is None:\n",
        "                    raise ValueError(f\"Failed to load data for PR #{pr_number_str}. Cannot perform RAG analysis.\")\n",
        "\n",
        "            # Get the retriever for the specific PR's database\n",
        "            retriever = self.pr_databases[pr_number_str].as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "            # Retrieve relevant documents based on the question\n",
        "            source_documents = retriever.get_relevant_documents(question)\n",
        "\n",
        "            # Format the retrieved documents into a context string\n",
        "            context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in source_documents])\n",
        "\n",
        "            # Get CI checks for the prompt (assuming they are in metadata)\n",
        "            pr_dir_name = f\"pr_{pr_number_str}\"\n",
        "            metadata = self._load_pr_metadata(os.path.join(self.data_path, pr_dir_name))\n",
        "            ci_checks_list = [c.get('name', 'N/A') for c in metadata.get('check_runs', []) if isinstance(c, dict)]\n",
        "            ci_checks_str = \", \".join(ci_checks_list) if ci_checks_list else \"No CI checks found.\"\n",
        "\n",
        "            # Define the prompt template manually\n",
        "            prompt_template = \"\"\"<|system|>\n",
        "            You are a helpful assistant specializing in code review analysis.\n",
        "            You are analyzing Pull Request #{pr_number}. Relevant context from the PR is provided below:\n",
        "\n",
        "            {context}\n",
        "\n",
        "            Consider the following aspects from the PR data:\n",
        "            1. Code changes (diff)\n",
        "            2. Developer comments (issue and review comments)\n",
        "            3. Results of CI checks: {ci_checks}\n",
        "            4. Commit history (summarized in context)\n",
        "\n",
        "            Based on the provided context, answer the user's question about the Pull Request.\n",
        "            If the context does not contain enough information to answer the question,\n",
        "            state that you cannot answer based on the available information.\n",
        "            </s>\n",
        "            <|user|>\n",
        "            {question}\n",
        "            </s>\n",
        "            <|assistant|>\n",
        "            \"\"\"\n",
        "\n",
        "            # --- Debugging Print Statements (can be removed later) ---\n",
        "            print(f\"--- Debugging Prompt Variables for PR #{pr_number_str} ---\")\n",
        "            print(f\"pr_number_str: {pr_number_str}\")\n",
        "            print(f\"question: {question}\")\n",
        "            print(f\"ci_checks_str: {ci_checks_str}\")\n",
        "            print(f\"context_text (first 200 chars): {context_text[:200]}...\")\n",
        "            print(\"--- End Debugging Print Statements ---\")\n",
        "\n",
        "\n",
        "            # Manually format the prompt string\n",
        "            formatted_prompt = prompt_template.format(\n",
        "                pr_number=pr_number_str,\n",
        "                context=context_text,\n",
        "                ci_checks=ci_checks_str,\n",
        "                question=question\n",
        "            )\n",
        "\n",
        "            # Invoke the LLM directly with the formatted prompt\n",
        "            # The LLM pipeline expects a single string input\n",
        "            llm_response = self.llm.invoke(formatted_prompt)\n",
        "\n",
        "            # The LLM response might contain the original prompt + the generated answer.\n",
        "            # We need to extract just the generated answer part.\n",
        "            # This extraction depends on the specific LLM's output format.\n",
        "            # For chat models, the assistant's response usually follows the <|assistant|> tag.\n",
        "            answer = llm_response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "\n",
        "            # Format the source documents for output\n",
        "            formatted_sources = self._format_sources(source_documents)\n",
        "\n",
        "            return {\n",
        "                \"pr\": pr_number_str, # Return as string for consistency\n",
        "                \"question\": question,\n",
        "                \"answer\": answer if answer else \"Could not generate an answer based on the available information.\",\n",
        "                \"sources\": formatted_sources\n",
        "            }\n",
        "        except ValueError as e:\n",
        "            # Handles errors from LLM not initialized or data loading failed\n",
        "            print(f\"Error getting review for PR #{pr_number_str}: {e}\")\n",
        "            return {\n",
        "                \"pr\": pr_number_str,\n",
        "                \"question\": question,\n",
        "                \"answer\": f\"Error processing PR #{pr_number_str}: {e}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred getting review for PR #{pr_number_str}: {str(e)}\")\n",
        "            return {\n",
        "                \"pr\": pr_number_str,\n",
        "                \"question\": question,\n",
        "                \"answer\": f\"An unexpected error occurred during review generation: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "\n",
        "    def _format_sources(self, docs):\n",
        "        \"\"\"\n",
        "        Formats source documents returned by the retriever for output.\n",
        "        Since metadata is not stored in Chroma, this will only show basic info\n",
        "        like the document content itself if the retriever returns Document objects.\n",
        "        Args:\n",
        "            docs (list): A list of source documents (expected to be Document objects without metadata).\n",
        "        Returns:\n",
        "            list: A list of formatted source dictionaries (will have limited info).\n",
        "        \"\"\"\n",
        "        formatted_sources = []\n",
        "        for doc in docs:\n",
        "            # When metadata is not stored, the retrieved 'doc' is typically a Document object\n",
        "            # with only 'page_content'.\n",
        "            # We can't get file, checks, or author from metadata if it wasn't stored.\n",
        "            source_info = {\n",
        "                \"content_snippet\": str(doc.page_content)[:200] + \"...\" if doc and hasattr(doc, 'page_content') else \"N/A\", # Show a snippet of the content\n",
        "                \"file\": \"Unknown (metadata not stored)\", # Indicate metadata is missing\n",
        "                \"checks\": \"Unknown (metadata not stored)\",\n",
        "                \"author\": \"Unknown (metadata not stored)\"\n",
        "            }\n",
        "            formatted_sources.append(source_info)\n",
        "        return formatted_sources\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Repository to analyze ---\n",
        "    # Replace with your target repository\n",
        "    owner = 'AlfaInsurance'\n",
        "    repo = 'devQ_testData_PythonProject'\n",
        "\n",
        "    # --- State of PRs to fetch ---\n",
        "    # 'all', 'open', 'closed'\n",
        "    pr_state = 'all'\n",
        "\n",
        "    # --- Data Fetching (Optional, only if data needs to be updated or is missing) ---\n",
        "    # Uncomment the block below if you need to fetch/update the data\n",
        "    print(f\"Starting STRUCTURED full pull request data fetch for {owner}/{repo}\")\n",
        "    print(f\"Target PR state: {pr_state}\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR_BASE}\")\n",
        "    print(\"Ensure GITHUB_BOT_ACCESS_TOKEN environment variable is set.\")\n",
        "    print(\"WARNING: This can take a long time and consume significant disk space and API calls.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Ensure github_secret is loaded from environment before fetching\n",
        "    if github_secret:\n",
        "        processed_prs = get_all_pull_requests_structured(\n",
        "            owner,\n",
        "            repo,\n",
        "            state=pr_state\n",
        "        )\n",
        "    else:\n",
        "        processed_prs = []\n",
        "        print(\"Skipping data fetch because GITHUB_BOT_ACCESS_TOKEN is not set.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    if processed_prs:\n",
        "        print(f\"\\n--------------------------------------------------\")\n",
        "        print(f\"Successfully finished processing.\")\n",
        "        print(f\"Processed {len(processed_prs)} pull requests.\")\n",
        "        print(f\"Data saved in '{OUTPUT_DIR_BASE}' directory, organized by PR number.\")\n",
        "        print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
        "        print(f\"--------------------------------------------------\")\n",
        "    else:\n",
        "        print(\"\\nNo pull requests processed during fetch.\")\n",
        "        print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "    # --- RAG Usage Example ---\n",
        "    print(\"\\n--- Starting RAG Analysis ---\")\n",
        "    rag_system = PRSpecificRAG()\n",
        "    rag_system.initialize_llm() # Initialize the LLM once\n",
        "\n",
        "    # Example PR numbers (should match folder names in OUTPUT_DIR_BASE)\n",
        "    # Make sure the data for these PRs exists in the specified OUTPUT_DIR_BASE\n",
        "    # If you uncommented the data fetching block above, this will use the fetched data.\n",
        "    # Otherwise, ensure the 'pull_request_data_structured' directory exists with data.\n",
        "    # Use the list of successfully processed PRs from the data fetch step\n",
        "    pr_numbers_to_analyze = [str(pr_num) for pr_num in processed_prs] # Use processed PRs as strings\n",
        "\n",
        "    questions = [\n",
        "        \"Какие потенциальные уязвимости есть в этих изменениях?\",\n",
        "        \"Соответствует ли код стандартам проекта?\",\n",
        "        \"Есть ли проблемы с производительностью в измененном коде?\",\n",
        "        \"Summarize the main changes in PR 1.\", # Example in English\n",
        "        \"What were the CI check results for PR 2?\" # Example in English\n",
        "    ]\n",
        "\n",
        "    if not pr_numbers_to_analyze:\n",
        "        print(\"No PRs were successfully processed during data fetch. Cannot perform RAG analysis.\")\n",
        "    else:\n",
        "        for pr_num in pr_numbers_to_analyze:\n",
        "            print(f\"\\n\\033[1m--- Analyzing PR #{pr_num} ---\\033[0m\")\n",
        "            try:\n",
        "                # Load data for the specific PR if not already loaded\n",
        "                # The get_review function now handles loading if needed\n",
        "                # Loop through questions for each PR\n",
        "                for q in questions:\n",
        "                     print(f\"\\nВопрос: {q}\")\n",
        "                     result = rag_system.get_review(pr_num, q)\n",
        "                     print(f\"Ответ: {result['answer']}\")\n",
        "                     print(f\"Источники: {result['sources']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during analysis of PR {pr_num}: {str(e)}\")\n",
        "\n",
        "    print(\"\\n--- RAG Analysis Finished ---\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7z6JFo7Rvmse"
      },
      "execution_count": 3,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}