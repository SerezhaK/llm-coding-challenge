import os
import time
import streamlit as st
from datetime import datetime
from dateutil.parser import isoparse  # Import for parsing ISO 8601 dates
import logging
from .core import fetch_github_data, CodeChangeRAG
from datetime import datetime, timezone
logger = logging.getLogger(__name__)

OUTPUT_DIR_BASE = "github_data_structured"
CODER_ANALYSIS_OUTPUT_DIR = "coder_analysis"  # New directory for saving coder analysis results
pr_state_to_fetch = 'closed'  # Fetch closed PRs for analysis


def code_review(
        github_owner,
        github_repo,
        coder_to_analyze_login,
        branch_for_merge_history='master',
        analysis_start_date_str="2025-01-01T00:00:00Z",
        analysis_end_date_str="2025-04-18T23:59:59Z"
):
    # --- Date Range for Filtering Changes for Analysis ---
    # Set the start and end dates for filtering changes for coder analysis
    # The format should be ISO 8601:YYYY-MM-DDTHH:MM:SSZ
    # Example: "2024-01-01T00:00:00Z"
    # try:
    #     analysis_start_date = isoparse(analysis_start_date_str) if analysis_start_date_str else None
    #     analysis_end_date = isoparse(analysis_end_date_str) if analysis_end_date_str else None
    #     if analysis_start_date and analysis_end_date and analysis_start_date > analysis_end_date:
    #         print("Warning: analysis_start_date is after analysis_end_date. No changes will be analyzed.")
    #         analysis_start_date = analysis_end_date = None  # Invalidate date range if illogical
    # except ValueError as e:
    #     print(
    #         f"Error parsing date strings: {e}. Please ensure dates are in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ). Skipping date filtering for analysis.")
    #     analysis_start_date = None
    #     analysis_end_date = None

    # --- Data Fetching ---

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≤–∏–¥–µ markdown –≤ Streamlit
    st.markdown(f"""
    ### –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è:  
    **–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:** `{github_owner}/{github_repo}`  
    **–°–æ—Å—Ç–æ—è–Ω–∏–µ PR –¥–ª—è –≤—ã–±–æ—Ä–∫–∏:** `{pr_state_to_fetch}`  
    **–í–µ—Ç–∫–∞ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ —Å–ª–∏—è–Ω–∏–π:** `{branch_for_merge_history}`  
    **–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:** `{analysis_start_date_str}` –¥–æ `{analysis_end_date_str}`   

    ‚ö†Ô∏è **–í–Ω–∏–º–∞–Ω–∏–µ:**  
    - –ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏  
    - –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ –∏ API-–∑–∞–ø—Ä–æ—Å—ã  
    """)

    # Pass the date range to fetch_github_data for filtering merge commits during fetch
    # Note: PR fetching is NOT filtered by date here, only merge history is.
    # PRs will be filtered by date AFTER fetching.
    fetched_pr_data, fetched_merge_history = fetch_github_data(
        github_owner,
        github_repo,
        pr_state=pr_state_to_fetch,
        branch_for_merge_history=branch_for_merge_history,
        merge_history_since=analysis_start_date_str,  # Use analysis date range for fetching merge commits
        merge_history_until=analysis_end_date_str  # Use analysis date range for fetching merge commits
    )

    start_time = time.time()
    end_time = time.time()

    execution_time = end_time - start_time

    if fetched_pr_data or fetched_merge_history:
        # –£—Å–ø–µ—à–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏
        st.success("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö!")

        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="–ó–∞–≥—Ä—É–∂–µ–Ω–æ PR", value=len(fetched_pr_data))
        with col2:
            st.metric(label="–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–º–∏—Ç–æ–≤", value=len(fetched_merge_history))

        st.info(f"**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {execution_time:.2f} —Å–µ–∫—É–Ω–¥")

        with st.expander("–î–µ—Ç–∞–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"):
            st.write(f"üìÅ **–ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏:** `{OUTPUT_DIR_BASE}`")
            st.write(f"‚è± **–ù–∞—á–∞–ª–æ:** `{datetime.fromtimestamp(start_time)}`")
            st.write(f"‚è± **–û–∫–æ–Ω—á–∞–Ω–∏–µ:** `{datetime.fromtimestamp(end_time)}`")

    else:

        st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
        st.info(f"**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {execution_time:.2f} —Å–µ–∫—É–Ω–¥")

        with st.expander("–î–µ—Ç–∞–ª–∏"):
            st.write(f"‚è± **–ù–∞—á–∞–ª–æ:** `{datetime.fromtimestamp(start_time)}`")
            st.write(f"‚è± **–û–∫–æ–Ω—á–∞–Ω–∏–µ:** `{datetime.fromtimestamp(end_time)}`")
            st.write("‚ÑπÔ∏è –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç—É–ø–æ–º")

    # --- Filter Fetched Changes by Date and Coder for Analysis ---
    changes_for_coder_analysis = []
    all_fetched_changes = fetched_pr_data + fetched_merge_history

    logging.info(
        f"\nFiltering fetched changes by date ({analysis_start_date_str} to {analysis_end_date_str}) and coder ({coder_to_analyze_login}) for analysis...")
    for change in all_fetched_changes:
        change_type = change.get('request_type')
        change_id = change.get('request_id')
        change_date_str = None
        author_login = None
        committer_login = None
        merged_by_login = None

        # Determine the relevant date and author/committer based on change type
        if change_type == 'pr':
            change_date_str = change.get('merged_at') or change.get('closed_at') or change.get('updated_at')
            author_login = change.get('author_login')
            merged_by_login = change.get('merged_by_login')
        elif change_type == 'merge_commit':
            change_date_str = change.get('committer_date') or change.get('author_date')
            author_login = change.get('api_author_login')
            committer_login = change.get('api_committer_login')

        if change_date_str:
            try:
                start_date = datetime.fromisoformat(analysis_start_date_str).replace(tzinfo=timezone.utc)
                end_date = datetime.fromisoformat(analysis_end_date_str).replace(tzinfo=timezone.utc)
                change_date = isoparse(change_date_str).astimezone(timezone.utc)  # Convert to UTC

                # Check if the change is within the date range AND the coder is the author, committer, or merger
                if start_date <= change_date <= end_date:
                    if author_login in coder_to_analyze_login or \
                            committer_login in coder_to_analyze_login or \
                            merged_by_login in coder_to_analyze_login:
                        changes_for_coder_analysis.append(change)
            except ValueError:
                logging.warning(
                    f"Warning: Could not parse date '{change_date_str}' for {change_type.upper()} {change_id}. Skipping filtering for this change.")
                pass  # Skip if date parsing fails
    logging.info(
        f"Found {len(changes_for_coder_analysis)} changes (PRs and Merge Commits) by {coder_to_analyze_login} within the specified date range for analysis.")

    # --- Coder Activity Analysis ---
    if not changes_for_coder_analysis:
        print(f"No changes found for coder {coder_to_analyze_login} within the specified date range.")
    else:
        # Initialize RAG system
        rag_system = CodeChangeRAG(data_path=OUTPUT_DIR_BASE)
        rag_system.initialize_llm()  # Initialize the LLM once

        if rag_system.llm is None:
            print("LLM was not initialized. Cannot perform RAG analysis for coder activity.")
        else:
            # Perform the coder activity analysis using the RAG system
            coder_analysis_results = rag_system.analyze_coder_activity(
                coder_to_analyze_login,
                changes_for_coder_analysis
            )

            # Create directory to save coder analysis results
            os.makedirs(CODER_ANALYSIS_OUTPUT_DIR, exist_ok=True)
            print(f"\nSaving coder analysis results to directory: {CODER_ANALYSIS_OUTPUT_DIR}")

    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∞–Ω–∞–ª–∏–∑–∞
    st.header(f"üìä –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞: {coder_to_analyze_login}")
    st.caption(
        f"–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {github_owner}/{github_repo} ‚Ä¢ –ü–µ—Ä–∏–æ–¥: {analysis_start_date_str} ‚Äî {analysis_end_date_str}")

    # –†–∞–∑–¥–µ–ª —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    with st.container(border=True):
        st.subheader("üìà –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")

        cols = st.columns(4)
        cols[0].metric("–í—Å–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π", coder_analysis_results.get('total_changes_analyzed', 0))
        cols[1].metric("–í—Å–µ–≥–æ –∫–æ–º–º–∏—Ç–æ–≤", coder_analysis_results.get('total_commits', 0))
        cols[2].metric("–î–æ–±–∞–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫", coder_analysis_results.get('total_additions', 0))
        cols[3].metric("–£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫", coder_analysis_results.get('total_deletions', 0))

    # –†–∞–∑–¥–µ–ª —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
    st.subheader("üß† –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (RAG)")
    rag_analysis = coder_analysis_results.get('analysis_results')

    if isinstance(rag_analysis, dict):
        tabs = st.tabs([f"–í–æ–ø—Ä–æ—Å {i + 1}" for i in range(len(rag_analysis))])

        for i, (question, result) in enumerate(rag_analysis.items()):
            with tabs[i]:
                st.markdown(f"**‚ùì –í–æ–ø—Ä–æ—Å:** {question}")
                st.markdown(f"**üí° –û—Ç–≤–µ—Ç:** {result.get('answer', 'N/A')}")

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                with st.expander("üîç –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
                    if result.get('sources'):
                        for j, source in enumerate(result['sources']):
                            st.markdown(f"**–ò—Å—Ç–æ—á–Ω–∏–∫ {j + 1}:**\n{source.get('content_snippet', 'N/A')}")
                    else:
                        st.info("–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    else:
        st.warning(str(rag_analysis))

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫—É –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ (–µ—Å–ª–∏ –≤—Å–µ –∂–µ –Ω—É–∂–Ω–æ)
    if st.button("üì• –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –≤ PDF", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª"):
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç–∫—Å–ø–æ—Ä—Ç –≤ PDF
        st.toast("–≠–∫—Å–ø–æ—Ä—Ç –≤ PDF –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω", icon="‚ö†Ô∏è")

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    st.toast(f"–ê–Ω–∞–ª–∏–∑ –¥–ª—è {coder_to_analyze_login} —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!", icon="‚úÖ")

    print("\n--- Script Finished ---")
